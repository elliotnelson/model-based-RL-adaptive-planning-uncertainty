## Model-based Reinforcement Learning with Adaptive Planning under Uncertainty

This project aims to train an RL agent to plan adaptively with a perfect model by using its observations to evaluate its model's quality locally when making and using predictions in different regions of state space.

The current implementation augments a baseline model-free A2C (Advantage Actor Critic) algorithm with a model-based loop which trains the transition (and/or reward) model at each iteration, and uses it to train the policy on model rollouts.

When training the policy on simulated model rollouts, we allow the horizon or length of rollouts to be selected adaptively. We furthermore adapt the horizon of each rollout locally in state space (instead of using a uniform, globally adaptive horizon for all rollouts). This is done by estimating the deviation of a model rollout from the true dynamics and rewards. Because we're using rollouts to update the policy, the error metric which matters is the deviation of the gradient update to the policy (or value function). In particular, we use the difference
<a href="https://www.codecogs.com/eqnedit.php?latex=||\vec{\hat{g}}(s_{real})&space;-&space;\vec{g}(s_{sim})||" target="_blank"><img src="https://latex.codecogs.com/gif.latex?||\vec{\hat{g}}(s_{real})&space;-&space;\vec{g}(s_{sim})||" title="||\vec{\hat{g}}(s_{real}) - \vec{g}(s_{sim})||" /></a>
between the policy gradient update <a href="https://www.codecogs.com/eqnedit.php?latex=\vec{g}(s_{sim})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vec{g}(s_{sim})" title="\vec{g}(s_{sim})" /></a> from a partial simulated rollout, and an estimate <a href="https://www.codecogs.com/eqnedit.php?latex=\vec{\hat{g}}(s_{real})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vec{\hat{g}}(s_{real})" title="\vec{\hat{g}}(s_{real})" /></a>  of the corresponding ground-truth policy gradient update obtained by looking up the real transition data (in a replay buffer) which is closest to the simulated rollout. 
